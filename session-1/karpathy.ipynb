{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('thesis_env': venv)"
  },
  "interpreter": {
   "hash": "2c8570d5e2d82a34cb4b898b661982eea3f59281313cb12bc1a914245ee63b4f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-6\n"
     ]
    }
   ],
   "source": [
    "def forwardMultiplyGate(x, y):\n",
    "    return x * y\n",
    "\n",
    "x = -2\n",
    "y = 3\n",
    "print(forwardMultiplyGate(x, y))"
   ]
  },
  {
   "source": [
    "### The Goal\n",
    "The problem we are interested in studying looks as follows:\n",
    "\n",
    "We provide a given circuit some specific input values (e.g. x = -2, y = 3)\n",
    "The circuit computes an output value (e.g. -6)\n",
    "The core question then becomes: How should one tweak the input slightly to increase the output?\n",
    "In this case, in what direction should we change x,y to get a number larger than -6? Note that, for example, x = -1.99 and y = 2.99 gives x * y = -5.95, which is higher than -6.0. Don’t get confused by this: -5.95 is better (higher) than -6.0. It’s an improvement of 0.05, even though the magnitude of -5.95 (the distance from zero) happens to be lower."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Strategy #1: Random Local Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweakAmount = 0.01\n",
    "best = -np.inf\n",
    "bestX = x\n",
    "bestY = y\n",
    "\n",
    "for k in range(0, 100):\n",
    "    x_try = x + tweakAmount * (np.random.uniform() * 2 - 1)\n",
    "    y_try = y + tweakAmount * (np.random.uniform() * 2 - 1)\n",
    "    out = forwardMultiplyGate(x_try, y_try)\n",
    "    if out > best:\n",
    "        best = out\n",
    "        bestX = x_try\n",
    "        bestY = y_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best x: -1.990797610736487\nBest y: 2.9907113917947616\nBest out: -5.9539010931874055\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best x: {bestX}\")\n",
    "print(f\"Best y: {bestY}\")\n",
    "print(f\"Best out: {best}\")"
   ]
  },
  {
   "source": [
    "#### This is a perfectly fine strategy for tiny problems with a few gates if you can afford the compute time, but it won’t do if we want to eventually consider huge circuits with millions of inputs. It turns out that we can do much better."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Strategy #2: Numerical Gradient\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_deriv: 3.00000000000189\nY_deriv: -2.0000000000042206\nX_deriv out: -5.9997\nY_deriv out: -6.0002\n"
     ]
    }
   ],
   "source": [
    "x = -2\n",
    "y = 3\n",
    "out = forwardMultiplyGate(x, y)\n",
    "h = 0.0001\n",
    "\n",
    "# Compute derivative with respect to x\n",
    "xph = x + h\n",
    "out2 = forwardMultiplyGate(xph, y)\n",
    "x_derivative = (out2 - out) / h\n",
    "\n",
    "# compute derivative with respect to y\n",
    "yph = y + h\n",
    "out3 = forwardMultiplyGate(x, yph)\n",
    "y_derivative = (out3 - out) / h\n",
    "\n",
    "print(f\"X_deriv: {x_derivative}\")\n",
    "print(f\"Y_deriv: {y_derivative}\")\n",
    "\n",
    "print(f\"X_deriv out: {out2}\")\n",
    "print(f\"Y_deriv out: {out3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-1.969999999999981\n2.979999999999958\n-5.87059999999986\n"
     ]
    }
   ],
   "source": [
    "stepSize = 0.01\n",
    "out = forwardMultiplyGate(x, y)  # -6\n",
    "x = x + stepSize * x_derivative\n",
    "y = y + stepSize * y_derivative\n",
    "outNew = forwardMultiplyGate(x, y)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(outNew)"
   ]
  },
  {
   "source": [
    "## Strategy #3: Analytic Gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-1.97\n2.98\n-5.8706\n"
     ]
    }
   ],
   "source": [
    "x = -2\n",
    "y = 3\n",
    "out = forwardMultiplyGate(x, y)\n",
    "x_gradient = y\n",
    "y_gradient = x\n",
    "\n",
    "stepSize = 0.01\n",
    "x += stepSize * x_gradient\n",
    "y += stepSize * y_gradient\n",
    "outNew = forwardMultiplyGate(x, y)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(outNew)"
   ]
  },
  {
   "source": [
    "## Recursive Case: Circuits with Multiple Gates\n",
    "\n",
    "The expression we are computing now is f(x,y,z)=(x+y)z."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-12\n"
     ]
    }
   ],
   "source": [
    "def forwardMultiplyGate(a, b):\n",
    "    return a * b\n",
    "\n",
    "def forwardAddGate(a, b):\n",
    "    return a + b\n",
    "\n",
    "def forwardCircuit(x, y, z):\n",
    "    q = forwardAddGate(x, y)\n",
    "    f = forwardMultiplyGate(q, z)\n",
    "    return f\n",
    "\n",
    "x = -2\n",
    "y = 5\n",
    "z = -4\n",
    "f = forwardCircuit(x, y, z)\n",
    "print(f)"
   ]
  },
  {
   "source": [
    "### Backpropagation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-4.0\n-4.0\n"
     ]
    }
   ],
   "source": [
    "# Initial conditions\n",
    "x = -2\n",
    "y = 5\n",
    "z = -4\n",
    "q = forwardAddGate(x, y)\n",
    "f = forwardMultiplyGate(q, z)\n",
    "\n",
    "# gradient of the MULTIPLY gate with respect to its inputs\n",
    "derivative_f_wrt_z = q  # 3\n",
    "derivative_f_wrt_q = z  # -4\n",
    "\n",
    "# derivative of the ADD gate with respect to its inputs\n",
    "derivative_q_wrt_x = 1.0\n",
    "derivative_q_wrt_y = 1.0\n",
    "\n",
    "# chain rule\n",
    "derivative_f_wrt_x = derivative_q_wrt_x * derivative_f_wrt_q  # -4\n",
    "derivative_f_wrt_y = derivative_q_wrt_y * derivative_f_wrt_q  # -4\n",
    "print(derivative_f_wrt_x)\n",
    "print(derivative_f_wrt_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "new x: -2.04\nnew y: 4.96\nnew z: -3.97\nnew forwardAddGate out: 2.92\nnew forwardMultiplyGate out: -11.5924\n"
     ]
    }
   ],
   "source": [
    "# final gradient from above: [-4, -4, 3]\n",
    "gradient_f_wrt_xyz = [derivative_f_wrt_x, derivative_f_wrt_y, derivative_f_wrt_z]\n",
    "\n",
    "# leth the inputs respond to the force/tug:\n",
    "stepSize = 0.01\n",
    "x = x + stepSize * derivative_f_wrt_x\n",
    "y = y + stepSize * derivative_f_wrt_y\n",
    "z = z + stepSize * derivative_f_wrt_z\n",
    "print(f\"new x: {x}\")\n",
    "print(f\"new y: {y}\")\n",
    "print(f\"new z: {z}\")\n",
    "\n",
    "# Our circuit noew better give higher output:\n",
    "q = forwardAddGate(x, y)\n",
    "f = forwardMultiplyGate(q, z)\n",
    "print(f\"new forwardAddGate out: {q}\")\n",
    "print(f\"new forwardMultiplyGate out: {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-3.9999999999906777, -3.9999999999906777, 3.000000000010772]\n"
     ]
    }
   ],
   "source": [
    "# Numerical gradient check\n",
    "x = -2\n",
    "y = 5\n",
    "z = -4\n",
    "\n",
    "# Numerical gradient check\n",
    "h = 0.0001\n",
    "x_derivative = (forwardCircuit(x + h, y, z) - forwardCircuit(x, y, z)) / h\n",
    "y_derivative = (forwardCircuit(x, y + h, z) - forwardCircuit(x, y, z)) / h\n",
    "z_derivative = (forwardCircuit(x, y, z + h) - forwardCircuit(x, y, z)) / h\n",
    "\n",
    "print([x_derivative, y_derivative, z_derivative])"
   ]
  },
  {
   "source": [
    "# Example Neuron"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a Unit corresponding to a wire in diagrams between gates\n",
    "class Unit:\n",
    "    def __init__(self, value, grad):\n",
    "        self.value = value\n",
    "        self.grad = grad\n",
    "\n",
    "\n",
    "class MultiplyGate:\n",
    "    def forward(self, u0, u1):\n",
    "        self.u0 = u0\n",
    "        self.u1 = u1\n",
    "        self.utop = Unit(u0.value * u1.value, 0.0)\n",
    "        return self.utop\n",
    "\n",
    "    def backward(self):\n",
    "        self.u0.grad += self.u1.value * self.utop.grad\n",
    "        self.u1.grad += self.u0.value * self.utop.grad\n",
    "\n",
    "\n",
    "class AddGate:\n",
    "    def forward(self, u0, u1):\n",
    "        self.u0 = u0\n",
    "        self.u1 = u1\n",
    "        self.utop = Unit(u0.value + u1.value, 0.0)\n",
    "        return self.utop\n",
    "\n",
    "    def backward(self):\n",
    "        self.u0.grad += 1 * self.utop.grad\n",
    "        self.u1.grad += 1 * self.utop.grad\n",
    "\n",
    "\n",
    "class SigmoidGate:\n",
    "    def sig(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, u0):\n",
    "        self.u0 = u0\n",
    "        self.utop = Unit(self.sig(self.u0.value), 0.0)\n",
    "        return self.utop\n",
    "\n",
    "    def backward(self):\n",
    "        s = self.sig(self.u0.value)\n",
    "        self.u0.grad += (s * (1 - s)) * self.utop.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8807970779778823\n"
     ]
    }
   ],
   "source": [
    "# Create input units\n",
    "a = Unit(1.0, 0.0)\n",
    "b = Unit(2.0, 0.0)\n",
    "c = Unit(-3.0, 0.0)\n",
    "x = Unit(-1.0, 0.0)\n",
    "y = Unit(3.0, 0.0)\n",
    "\n",
    "# Create the gates\n",
    "mulg0 = MultiplyGate()\n",
    "mulg1 = MultiplyGate()\n",
    "addg0 = AddGate()\n",
    "addg1 = AddGate()\n",
    "sg0 = SigmoidGate()\n",
    "\n",
    "# Forward pass\n",
    "def forwardNeuron(a, b, c, x, y):\n",
    "    ax = mulg0.forward(a, x)\n",
    "    by = mulg1.forward(b, y)\n",
    "    ax_by = addg0.forward(ax, by)\n",
    "    ax_by_c = addg1.forward(ax_by, c)\n",
    "    s = sg0.forward(ax_by_c)\n",
    "    return s\n",
    "\n",
    "s = forwardNeuron(a, b, c, x, y)\n",
    "print(s.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.grad = 1.0\n",
    "sg0.backward()    # writes gradient into ax_by_c\n",
    "addg1.backward()  # writes gradients into ax_by and c\n",
    "addg0.backward()  # writes gradients into ax and by\n",
    "mulg1.backward()  # writes gradients into b and y\n",
    "mulg0.backward()  # writes gradients into a and x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a.grad is: -0.10499358540350662\nb.grad is: 0.31498075621051985\nc.grad is: 0.10499358540350662\nx.grad is: 0.10499358540350662\ny.grad is: 0.20998717080701323\nCircuit output after one backprop: 0.8825501816218984\n"
     ]
    }
   ],
   "source": [
    "print(f\"a.grad is: {a.grad}\")\n",
    "print(f\"b.grad is: {b.grad}\")\n",
    "print(f\"c.grad is: {c.grad}\")\n",
    "print(f\"x.grad is: {x.grad}\")\n",
    "print(f\"y.grad is: {y.grad}\")\n",
    "\n",
    "stepSize = 0.01\n",
    "a.value += stepSize * a.grad \n",
    "b.value += stepSize * b.grad  \n",
    "c.value += stepSize * c.grad  \n",
    "x.value += stepSize * x.grad \n",
    "y.value += stepSize * y.grad \n",
    "\n",
    "s = forwardNeuron(a, b, c, x, y)\n",
    "print(f\"Circuit output after one backprop: {s.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.10499758359205913, 0.3149447748351797, 0.10498958734506125, 0.10498958734506125, 0.2099711788272618]\n"
     ]
    }
   ],
   "source": [
    "def forwardCircuitFast(a, b, c, x, y):\n",
    "    return 1 / (1 + np.exp(-(a*x + b*y + c)))\n",
    "\n",
    "a = 1\n",
    "b = 2\n",
    "c = -3\n",
    "x = -1\n",
    "y = 3\n",
    "h = 0.0001\n",
    "a_grad = (forwardCircuitFast(a+h, b, c, x, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "b_grad = (forwardCircuitFast(a, b+h, c, x, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "c_grad = (forwardCircuitFast(a, b, c+h, x, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "x_grad = (forwardCircuitFast(a, b, c, x+h, y) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "y_grad = (forwardCircuitFast(a, b, c, x, y+h) - forwardCircuitFast(a, b, c, x, y)) / h\n",
    "\n",
    "print([a_grad, b_grad, c_grad, x_grad, y_grad])"
   ]
  },
  {
   "source": [
    "# Machine Learning\n",
    "___\n",
    "## Binary Classification\n",
    "### Support Vector Machine"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A circuit: it takes 5 Units (x, y, a, b, c) and outputs a single Unit\n",
    "# It can also compute the gradient w.r.t its inputs\n",
    "class Circuit:\n",
    "    def __init__(self):\n",
    "        # Create some gates\n",
    "        self.mulg0 = MultiplyGate()\n",
    "        self.mulg1 = MultiplyGate()\n",
    "        self.addg0 = AddGate()\n",
    "        self.addg1 = AddGate()\n",
    "    \n",
    "    def forward(self, x, y, a, b, c):\n",
    "        self.ax = self.mulg0.forward(a, x)\n",
    "        self.by = self.mulg1.forward(b, y)\n",
    "        self.ax_by = self.addg0.forward(self.ax, self.by)\n",
    "        self.ax_by_c = self.addg1.forward(self.ax_by, c)\n",
    "        return self.ax_by_c\n",
    "    \n",
    "    def backward(self, gradientTop):  # Takes pull from above\n",
    "        self.ax_by_c.grad = gradientTop\n",
    "        self.addg1.backward()  # sets gradient in ax_by and c\n",
    "        self.addg0.backward()  # sets gradient in ax and by\n",
    "        self.mulg1.backward()  # sets gradient in b and y\n",
    "        self.mulg0.backward()  # sets gradient in a and x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self):\n",
    "        # Random initial parameter values\n",
    "        self.a = Unit(1.0, 0.0)\n",
    "        self.b = Unit(-2.0, 0.0)\n",
    "        self.c = Unit(-1.0, 0.0)\n",
    "        self.circuit = Circuit()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        self.unit_out = self.circuit.forward(x, y, self.a, self.b, self.c)\n",
    "        return self.unit_out\n",
    "    \n",
    "    def backward(self, label):  # label is +1/-1\n",
    "        # Reset pulls on a,b,c\n",
    "        self.a.grad = 0.0\n",
    "        self.b.grad = 0.0\n",
    "        self.c.grad = 0.0\n",
    "\n",
    "        # Compute the pull based on what the circuit output was\n",
    "        pull = 0.0\n",
    "        if label == 1 and self.unit_out.value < 1:\n",
    "            pull = 1\n",
    "        if label == -1 and self.unit_out.value > -1:\n",
    "            pull = -1\n",
    "        self.circuit.backward(pull)  # write gradient into x,y,a,b,c\n",
    "\n",
    "        # Add regularization pull for parameters: towards zero and proportional to value\n",
    "        self.a.grad += -self.a.value\n",
    "        self.b.grad += -self.b.value\n",
    "    \n",
    "    def learnFrom(self, x, y, label):\n",
    "        self.forward(x, y)  # forward pass (set .value in all Units)\n",
    "        self.backward(label)  # backward pass (set .grad in all Units)\n",
    "        self.parameterUpdate()  # parameters respond to tug\n",
    "    \n",
    "    def parameterUpdate(self):\n",
    "        stepSize = 0.01\n",
    "        self.a.value += stepSize * self.a.grad\n",
    "        self.b.value += stepSize * self.b.grad\n",
    "        self.c.value += stepSize * self.c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training accuracy at iteration 0: 0.3333333333333333\nTraining accuracy at iteration 25: 0.3333333333333333\nTraining accuracy at iteration 50: 0.3333333333333333\nTraining accuracy at iteration 75: 0.3333333333333333\nTraining accuracy at iteration 100: 0.3333333333333333\nTraining accuracy at iteration 125: 0.3333333333333333\nTraining accuracy at iteration 150: 0.3333333333333333\nTraining accuracy at iteration 175: 0.3333333333333333\nTraining accuracy at iteration 200: 0.3333333333333333\nTraining accuracy at iteration 225: 0.3333333333333333\nTraining accuracy at iteration 250: 0.3333333333333333\nTraining accuracy at iteration 275: 0.3333333333333333\nTraining accuracy at iteration 300: 0.3333333333333333\nTraining accuracy at iteration 325: 0.3333333333333333\nTraining accuracy at iteration 350: 0.3333333333333333\nTraining accuracy at iteration 375: 0.3333333333333333\nTraining accuracy at iteration 400: 0.3333333333333333\nTraining accuracy at iteration 425: 0.3333333333333333\nTraining accuracy at iteration 450: 0.3333333333333333\nTraining accuracy at iteration 475: 0.3333333333333333\nTraining accuracy at iteration 500: 0.3333333333333333\nTraining accuracy at iteration 525: 0.3333333333333333\nTraining accuracy at iteration 550: 0.3333333333333333\nTraining accuracy at iteration 575: 0.3333333333333333\nTraining accuracy at iteration 600: 0.3333333333333333\nTraining accuracy at iteration 625: 0.3333333333333333\nTraining accuracy at iteration 650: 0.3333333333333333\nTraining accuracy at iteration 675: 0.3333333333333333\nTraining accuracy at iteration 700: 0.3333333333333333\nTraining accuracy at iteration 725: 0.3333333333333333\nTraining accuracy at iteration 750: 0.3333333333333333\nTraining accuracy at iteration 775: 0.3333333333333333\nTraining accuracy at iteration 800: 0.3333333333333333\nTraining accuracy at iteration 825: 0.3333333333333333\nTraining accuracy at iteration 850: 0.3333333333333333\nTraining accuracy at iteration 875: 0.3333333333333333\nTraining accuracy at iteration 900: 0.3333333333333333\nTraining accuracy at iteration 925: 0.5\nTraining accuracy at iteration 950: 0.3333333333333333\nTraining accuracy at iteration 975: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "data.append([1.2, 0.7])\n",
    "labels.append(1)\n",
    "data.append([-0.3, -0.5])\n",
    "labels.append(-1)\n",
    "data.append([3.0, 0.1])\n",
    "labels.append(1)\n",
    "data.append([-0.1, -1.0])\n",
    "labels.append(-1)\n",
    "data.append([-1.0, 1.1])\n",
    "labels.append(-1)\n",
    "data.append([2.1, -3])\n",
    "labels.append(1)\n",
    "\n",
    "svm = SVM()\n",
    "\n",
    "# a function that computes the classification accuracy\n",
    "def evalTrainAcc():\n",
    "    num_correct = 0\n",
    "    for i in range(0, len(data)):\n",
    "        x = Unit(data[i][0], 0.0)\n",
    "        y = Unit(data[i][1], 0.0)\n",
    "        trueLabel = labels[i]\n",
    "\n",
    "        # see if the prediction matches the provided label\n",
    "        predictedLabel = svm.forward(x, y).value > 0 if 1 else -1\n",
    "        if predictedLabel == trueLabel:\n",
    "            num_correct += 1\n",
    "    return num_correct / len(data)\n",
    "\n",
    "import math\n",
    "\n",
    "# Training loop\n",
    "for k in range(0, 1000):\n",
    "    # Pick a random data point\n",
    "    i = math.floor(np.random.uniform() * len(data))\n",
    "    x = Unit(data[i][0], 0.0)\n",
    "    y = Unit(data[i][1], 0.0)\n",
    "    label = labels[i]\n",
    "    svm.learnFrom(x, y, label)\n",
    "\n",
    "    if k % 25 == 0:\n",
    "        print(f\"Training accuracy at iteration {k}: {evalTrainAcc()}\")\n"
   ]
  },
  {
   "source": [
    "## Loss function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "example 0: xi = ([1.2, 0.7]) and label = 1\nScore computed to be 0.56\n=> Cost computed to be 0.43999999999999995\nexample 1: xi = ([-0.3, 0.5]) and label = -1\nScore computed to be 0.37\n=> Cost computed to be 1.37\nexample 2: xi = ([3, 2.5]) and label = 1\nScore computed to be 1.1\n=> Cost computed to be 0.0\nRegularization cost for current model is 0.005000000000000001\nTotal cost is 1.815\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.815"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "X = [ [1.2, 0.7], [-0.3, 0.5], [3, 2.5] ]\n",
    "y = [1, -1, 1]\n",
    "w = [0.1, 0.2, 0.3]\n",
    "alpha = 0.1\n",
    "\n",
    "def cost(X, y, w):\n",
    "    totalCost = 0.0\n",
    "    N = len(X)\n",
    "    for i in range(0, N):\n",
    "        # Loop over all data points and compute their score\n",
    "        xi = X[i]\n",
    "        score = w[0] * xi[0] + w[1] * xi[1] + w[2]\n",
    "\n",
    "        # Accumulate cost based on how compatible the score is with the label\n",
    "        yi = y[i]\n",
    "        costi = np.max([0, -yi * score + 1])\n",
    "        print(f\"example {i}: xi = ({xi}) and label = {yi}\")\n",
    "        print(f\"Score computed to be {score}\")\n",
    "        print(f\"=> Cost computed to be {costi}\")\n",
    "        totalCost += costi\n",
    "\n",
    "    # Regularization cost: we want small weights\n",
    "    regCost = alpha * (w[0]*w[0] + w[1]*w[1])\n",
    "    print(f\"Regularization cost for current model is {regCost}\")\n",
    "    totalCost += regCost\n",
    "\n",
    "    print(f\"Total cost is {totalCost}\")\n",
    "    return totalCost\n",
    "\n",
    "cost(X, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}