<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<html>
<head>
	
	<meta http-equiv="content-type" content="text/html; charset=iso-8859-1"/>
	<title></title>
	<meta name="generator" content="https://conversiontools.io" />
	<meta name="created" content="2022-01-02T14:51:14"/>
	<meta name="changed" content="2022-01-27T10:11:52"/>
	<meta name="AppVersion" content="16.0300"/>
	
	<style type="text/css">
		body,div,table,thead,tbody,tfoot,tr,th,td,p { font-family:"Calibri"; font-size:x-small }
		a.comment-indicator:hover + comment { background:#ffd; position:absolute; display:block; border:1px solid black; padding:0.5em;  } 
		a.comment-indicator { background:red; display:inline-block; border:1px solid black; width:0.5em; height:0.5em;  } 
		comment { display:none;  } 
	</style>
	
</head>

<body>
<table cellspacing="0" border="0">
	<colgroup width="64"></colgroup>
	<colgroup width="240"></colgroup>
	<colgroup span="2" width="194"></colgroup>
	<colgroup width="166"></colgroup>
	<colgroup width="64"></colgroup>
	<colgroup width="165"></colgroup>
	<colgroup width="153"></colgroup>
	<colgroup width="223"></colgroup>
	<colgroup width="216"></colgroup>
	<colgroup width="188"></colgroup>
	<colgroup span="2" width="169"></colgroup>
	<colgroup width="171"></colgroup>
	<colgroup width="190"></colgroup>
	<colgroup width="466"></colgroup>
	<colgroup width="227"></colgroup>
	<tr>
		<td height="20" align="left" valign=bottom><font color="#000000">No.</font></td>
		<td align="left" valign=bottom><font color="#000000">1.Title</font></td>
		<td align="left" valign=bottom><font color="#000000">2.Paper Link</font></td>
		<td align="left" valign=bottom><font color="#000000">3. Domains</font></td>
		<td align="left" valign=bottom><font color="#000000">4. Github repository</font></td>
		<td align="left" valign=bottom><font color="#000000">5.Year</font></td>
		<td align="left" valign=bottom><font color="#000000">6.Authors</font></td>
		<td align="left" valign=bottom><font color="#000000">7.Affiliation</font></td>
		<td align="left" valign=bottom><font color="#000000">8.Generator Model</font></td>
		<td align="left" valign=bottom><font color="#000000">9.Discriminator Model</font></td>
		<td align="left" valign=bottom><font color="#000000">10. Encoder Model</font></td>
		<td align="left" valign=bottom><font color="#000000">11. Loss functions</font></td>
		<td align="left" valign=bottom><font color="#000000">12. Source Dataset</font></td>
		<td align="left" valign=bottom><font color="#000000">13.Target Dataset</font></td>
		<td align="left" valign=bottom><font color="#000000">14.Metrics used</font></td>
		<td align="left" valign=bottom><font color="#000000">15. Results</font></td>
		<td align="left" valign=bottom><font color="#000000">16.Further Research</font></td>
	</tr>
	<tr>
		<td height="380" align="right" valign=bottom sdval="1" sdnum="1033;"><font color="#000000">1</font></td>
		<td align="center" valign=middle><font color="#000000">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</font></td>
		<td align="center" valign=middle><u><font color="#0563C1"><a href="https://www.semanticscholar.org/paper/StarGAN%3A-Unified-Generative-Adversarial-Networks-Choi-Choi/302207c149bdf7beb6e46e4d4afbd2fa9ac02c64">https://www.semanticscholar.org/paper/StarGAN%3A-Unified-Generative-Adversarial-Networks-Choi-Choi/302207c149bdf7beb6e46e4d4afbd2fa9ac02c64</a></font></u></td>
		<td align="center" valign=middle><font color="#000000">Seven domains using<br>the following attributes: hair color (black, blond, brown),<br>gender (male/female), and age (young/old).<br></font></td>
		<td align="center" valign=middle><u><font color="#0563C1"><a href="https://github.com/yunjey/stargan">https://github.com/yunjey/stargan</a></font></u></td>
		<td align="center" valign=middle sdval="2018" sdnum="1033;"><font color="#000000">2018</font></td>
		<td align="center" valign=middle><font color="#000000">Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo</font></td>
		<td align="center" valign=middle><font color="#000000">Korea University, Clova AI Research, NAVER Corp, The College of New Jersey, Hong Kong University of Science &amp; Technology</font></td>
		<td align="center" valign=middle><font color="#000000">Adopted from CycleGAN. StarGAN has the generator network composed of two convolutional layers with the stride size of two for downsampling, six residual blocks, and two transposed convolutional layers with the stride size of two for upsampling. Instance normalization is used.</font></td>
		<td align="center" valign=middle><font color="#000000">PatchGANs are used for discriminator, which classifies whether local image patches are real or fake. No normalization is used here.</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
		<td align="center" valign=middle><font color="#000000">Wasserstein GAN objective with gradient penalty.</font></td>
		<td align="center" valign=middle><font color="#000000">CelebA dataset. Crop the initial 178 &times; 218 size<br>images to 178&times;178, then resize them as 128&times;128.</font></td>
		<td align="center" valign=middle><font color="#000000">RaFD (The Radboud Faces Database). Crop the images to 256 &times; 256, where the faces are cen-<br>tered, and then resize them to 128 &times; 128.</font></td>
		<td align="center" valign=middle><font color="#000000">Classifciation error. Number of parameters.</font></td>
		<td align="center" valign=middle><font color="#000000">Experimental results on CelebA: This method provides a higher visual quality of translation results on test data compared to the cross-domain models. For quantitative results survey was made using Amazon Mechanical Turk (AMT) for single and multiple attribute transfer tasks. In the case of single attribute transfer the difference was marginal between StarGAN and other models. E.g. 39.1% for StarGAN vs. 31.4% for DIAT. However in multi-attribute changes, e.g., the &lsquo;G+A&rsquo; (Gender + Angry) the performance difference becomes significant, e.g., 49.8% for StarGAN vs. 20.3% for IcGAN), clearly showing the advantages of StarGAN in more complicated, multi-attribute transfer tasks. Experimental Results on RaFD: StarGAN generates the most natural-looking expressions while properly maintaining the personal identity and facial features of the input. While DIAT and CycleGAN mostly preserve the identity of the input, many of their results are shown blurry and do not maintain the degree of sharpness.Experimental Results on CelebA+RaFD: it's demonstrated that StarGAN model not only learns from multiple domains within signle dataset but also from multiple datasets. StarGAN trained on multiple datasets can improve low-level tasks such as facial keypoint detection and segmentation.</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
	</tr>
	<tr>
		<td height="240" align="right" valign=bottom sdval="2" sdnum="1033;"><font color="#000000">2</font></td>
		<td align="center" valign=middle><font color="#000000">StarGAN v2: Diverse Image Synthesis for Multiple Domains</font></td>
		<td align="center" valign=middle><font color="#000000"><a href="https://www.semanticscholar.org/paper/StarGAN-v2%3A-Diverse-Image-Synthesis-for-Multiple-Choi-Uh/1045aee009c2a66e1330ac4ccfd22ec27665eebe">
https://www.semanticscholar.org/paper/StarGAN-v2%3A-Diverse-Image-Synthesis-for-Multiple-Choi-Uh/1045aee009c2a66e1330ac4ccfd22ec27665eebe</a></font></td>
		<td align="center" valign=middle><font color="#000000"><br></font></td>
		<td align="center" valign=middle><font color="#000000"><a href="https://github.com/clovaai/stargan-v2">https://github.com/clovaai/stargan-v2</a></font></td>
		<td align="center" valign=middle sdval="2020" sdnum="1033;"><font color="#000000">2020</font></td>
		<td align="center" valign=middle><font color="#000000">Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha</font></td>
		<td align="center" valign=middle><font color="#000000">Clova AI Research, NAVER Corp. EPFL</font></td>
		<td align="center" valign=middle><font color="#000000"><br>Generator uses Residual blocks with AvgPool as downsample and Upsample for upsampling. Instance normalization is used for first half of generator network and Adaptive Instance Norm used in other half. See p.12 in paper for full architecture.</font></td>
		<td align="center" valign=middle><font color="#000000">Discriminator is a multi-task discriminator, which contains multiple linear output branches. Discriminator contains six pre-activation residual blocks with leaky ReLU. K fully-connected layers are used for real/fake classification of each domain (Where K indicates the number of domains). See page 12 in paper.</font></td>
		<td align="center" valign=middle><font color="#000000">Mapping Network: Given a latent code z and a domain y, mapping network F generates a style code s. F consists of an MLP with multiple output branches to provide style codes for all available domains. Style Encoder: Given an image x and its corresponding domain y, encoder E extracts the style code s of x.</font></td>
		<td align="center" valign=middle><font color="#000000">Adversarial objective. Style Reconstruction. Style diversification. Cycle consistency loss.</font></td>
		<td align="center" valign=middle><font color="#000000"><br></font></td>
		<td align="center" valign=middle><font color="#000000">CelebA-HQ, AFHQ (Animal Faces)</font></td>
		<td align="center" valign=middle><font color="#000000">Frechet inception distance (FID). Learned perceptual image patch similarity (LIPS).</font></td>
		<td align="center" valign=middle><font color="#000000">StarGANv2 is able to produce diverse synthesis on source image using styles from different domains. Individual components of StarGANv2 were analyzed to show importance of each in whole network. Basic setup of StarGAN was able to make only local changes to source image while adding multi-task discriminator allowed generator to transform global structure of image. Results on CelebA-HQ dataset: FID score of 13.7 and LPIPS score of 0.452. Results on AFHQ dataset: FID 16.2 and LPIPS 0.450.</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
	</tr>
	<tr>
		<td height="220" align="right" valign=bottom sdval="3" sdnum="1033;"><font color="#000000">3</font></td>
		<td align="center" valign=middle><font color="#000000">A Style-Based Generator Architecture for Generative Adversarial Networks</font></td>
		<td align="center" valign=middle><u><font color="#0563C1"><a href="https://www.semanticscholar.org/paper/A-Style-Based-Generator-Architecture-for-Generative-Karras-Laine/ceb2ebef0b41e31c1a21b28c2734123900c005e2">https://www.semanticscholar.org/paper/A-Style-Based-Generator-Architecture-for-Generative-Karras-Laine/ceb2ebef0b41e31c1a21b28c2734123900c005e2</a></font></u></td>
		<td align="center" valign=middle><u><font color="#0563C1"><br></font></u></td>
		<td align="center" valign=middle><u><font color="#0563C1"><a href="https://github.com/NVlabs/stylegan">https://github.com/NVlabs/stylegan</a></font></u></td>
		<td align="center" valign=middle sdval="2019" sdnum="1033;"><font color="#000000">2019</font></td>
		<td align="center" valign=middle><font color="#000000">Tero Karras, Samuli Laine, Timo Aila</font></td>
		<td align="center" valign=middle><font color="#000000">NVIDIA</font></td>
		<td align="center" valign=middle><font color="#000000">Network uses Style based generator. Latent vector z is passed through 8MLP Mapping network f. Learned affine transformations then specialize w to styles y=(ys, yb) that control adaptive instance normalization operations after each convolution layer of the sytnhesis network g. Synthesis network starts from learned 4x4x512 constant tensor.</font></td>
		<td align="center" valign=middle><font color="#000000">Adopted from Progressive GAN.</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
		<td align="center" valign=middle><font color="#000000">WGAN-GP for CelebA-HQ dataset. WGAN-GP for FFHQ for configuration A and non-saturating loss with R1 regularization for configurations B-F.</font></td>
		<td align="center" valign=middle><font color="#000000"><br></font></td>
		<td align="center" valign=middle><font color="#000000">CelebA-HQ, FFHQ.</font></td>
		<td align="center" valign=middle><font color="#000000">Frechet inception distance (FID).</font></td>
		<td align="center" valign=middle><font color="#000000">Results, Frechet inception distance (FID) for various generator designs (lower is better).  (A) Baseline Progressive GAN: CelebA-HQ/7.79, FFHQ/8.04. (B) +Tuning (incl. bilinear up/down): CelebA-HQ/6.11, FFHQ/5.25. (C) +Add mapping and styles: CelebA-HQ/5.34, FFHQ/4.85. (D) +Remove traditional input: CelebA-HQ/5.07, FFHQ/4.88. (E) +Add noise inputs: CelebA-HQ/5.06, FFHQ/4.42. (F) +Mixing regularization: CelebA-HQ/5.17, FFHQ/4.40.</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
	</tr>
	<tr>
		<td height="180" align="right" valign=bottom sdval="4" sdnum="1033;"><font color="#000000">4</font></td>
		<td align="center" valign=middle><font color="#000000">Face aging with Conditional GANs</font></td>
		<td align="center" valign=middle><u><font color="#0563C1"><a href="https://www.semanticscholar.org/paper/Face-aging-with-conditional-generative-adversarial-Antipov-Baccouche/bc81a63ee07fde8f1d89befbd9fc26fdf0eae992">https://www.semanticscholar.org/paper/Face-aging-with-conditional-generative-adversarial-Antipov-Baccouche/bc81a63ee07fde8f1d89befbd9fc26fdf0eae992</a></font></u></td>
		<td align="center" valign=middle><u><font color="#0563C1"><br></font></u></td>
		<td align="center" valign=middle><font color="#000000">Not specified</font></td>
		<td align="center" valign=middle sdval="2017" sdnum="1033;"><font color="#000000">2017</font></td>
		<td align="center" valign=middle><font color="#000000">Grigory Antipov, Moez Baccouche, Jean-Luc Dugelay</font></td>
		<td align="center" valign=middle><font color="#000000">Orange Labs, France</font></td>
		<td align="center" valign=middle><font color="#000000">Same as DCGAN</font></td>
		<td align="center" valign=middle><font color="#000000">Same as DCGAN</font></td>
		<td align="center" valign=middle><font color="#000000">Encoder that approximates the inverse mapping of an input image x with attributes y to a latent vector z.</font></td>
		<td align="center" valign=middle><font color="#000000">Adversarial objective. </font></td>
		<td align="center" valign=middle><font color="#000000"><br></font></td>
		<td align="center" valign=middle><font color="#000000">IMDB-Wiki</font></td>
		<td align="center" valign=middle><font color="#000000">Face Recognition Score</font></td>
		<td align="center" valign=middle><font color="#000000">Reconstruction type (Initial Reconstruction z0): FR score/53.2%. Reconstruction type (&quot;Pixelwise&quot; Optimization z*pixel): FR score/59.8%. Reconstruction type (&quot;Identity Preserving&quot; Optimization z*IP): FR score/82.9%.</font></td>
		<td align="center" valign=middle><font color="#000000">This method can be used for synthetic augmentation of face datasets and for improving the robustness of face recognition solutions in cross-age scenarios. Face reconstruction part can be improved by combining &quot;Pixelwise&quot; and &quot;Identity-Preserving&quot; approaches into one optimization objective.</font></td>
	</tr>
	<tr>
		<td height="240" align="right" valign=bottom sdval="5" sdnum="1033;"><font color="#000000">5</font></td>
		<td align="center" valign=middle><font color="#000000">Learning Residual Images For Face attribute manipulation</font></td>
		<td align="center" valign=middle><u><font color="#0563C1"><a href="https://www.semanticscholar.org/paper/Learning-Residual-Images-for-Face-Attribute-Shen-Liu/5bb8d7e709d493102c027601b779eb5284a04a4d">https://www.semanticscholar.org/paper/Learning-Residual-Images-for-Face-Attribute-Shen-Liu/5bb8d7e709d493102c027601b779eb5284a04a4d</a></font></u></td>
		<td align="center" valign=middle><u><font color="#0563C1"><br></font></u></td>
		<td align="center" valign=middle><font color="#000000">Not specified</font></td>
		<td align="center" valign=middle sdval="2017" sdnum="1033;"><font color="#000000">2017</font></td>
		<td align="center" valign=middle><font color="#000000">Wei Shen, Rujie Liu</font></td>
		<td align="center" valign=middle><font color="#000000">Fujitsu Research &amp; Development Center, Beijing China</font></td>
		<td align="center" valign=middle><font color="#000000">Two generators which perform the inverse attribute manipulation (i.e. adding glasses and removing glasses). Both generators produce residual images with reference to input images.</font></td>
		<td align="center" valign=middle><font color="#000000">The Discriminator network is a three-category classifier that classifies images from different categories (i.e. images generated from G0 and G1, images with positive attribute labels, and images with negative attribute labels). L-1 norm regularization is applied to let the residual image be sparse.</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
		<td align="center" valign=middle><font color="#000000">Discriminator: Negative log of softmax probabilities over class labels. Generator: Adversarial objective. Perceptual loss is used to measure the content difference. Dual learning process is adopted between both Generator networks. L-1 norm regularization for residual image.</font></td>
		<td align="center" valign=middle><font color="#000000"><br></font></td>
		<td align="center" valign=middle><font color="#000000">CelebA, Labeled Faces in the Wild.</font></td>
		<td align="center" valign=middle><font color="#000000">Average normalized distance error from the landmark detection algorithm.</font></td>
		<td align="center" valign=middle><font color="#000000">Quantified effectiviness of glasses removal by the performance gain of face landmark detection accuracy. Three datasets are considered: D0 dataset containing images without glasses, D1 dataset containing images wearing glasses and D1m containing same images as D1 while those images are processed with glass removal using the proposed method. Eye Landmark: D0/0.02341, D1/0.03570, D1m/0.03048.</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
	</tr>
	<tr>
		<td height="160" align="right" valign=bottom sdval="6" sdnum="1033;"><font color="#000000">6</font></td>
		<td align="center" valign=middle><font color="#000000">Unsupervised Visual Attribute Transfer with Reconfigurable Generative Adversarial Networks</font></td>
		<td align="center" valign=middle><u><font color="#0563C1"><a href="https://www.semanticscholar.org/paper/Unsupervised-Visual-Attribute-Transfer-with-Kim-Kim/ebf163a71084bc2e65b4ebbacf8ce0313fdf2c56">https://www.semanticscholar.org/paper/Unsupervised-Visual-Attribute-Transfer-with-Kim-Kim/ebf163a71084bc2e65b4ebbacf8ce0313fdf2c56</a></font></u></td>
		<td align="center" valign=middle><u><font color="#0563C1"><br></font></u></td>
		<td align="center" valign=middle><font color="#000000">Not specified</font></td>
		<td align="center" valign=middle sdval="2017" sdnum="1033;"><font color="#000000">2017</font></td>
		<td align="center" valign=middle><font color="#000000">Taeksoo Kim, Byoungjip Kim, Moonsu Cha, Jiwon Kim</font></td>
		<td align="center" valign=middle><font color="#000000">SK T-Brain</font></td>
		<td align="center" valign=middle><font color="#000000">One generator for Transfer part of network and one generator for Back-transfer part.</font></td>
		<td align="center" valign=middle><font color="#000000">Discriminator encourages generated images to be indistinguishable from real images with attribute, e.g. bang hair.</font></td>
		<td align="center" valign=middle><font color="#000000">Similar to generators there is Encoder for Transfer part and for Back-transfer. Encoder takes an image as input and encodes it into attribute features.</font></td>
		<td align="center" valign=middle><font color="#000000">For Generator and Encoder: Transfer objective. Back-transfer objective, inspired by cycle-consistency loss. Attribute consistency objective. For Discriminator: Discriminator objective of GAN.</font></td>
		<td align="center" valign=middle><font color="#000000"><br></font></td>
		<td align="center" valign=middle><font color="#000000">CelebA, 3D Car and 3D Face for Angle/Object visual attribute experiment.</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
		<td align="center" valign=middle><font color="#000000">Instance-level attribute transfer: details of the reference image are successfully transferred into the input image. Domain-level Attribute transfer: proposed method successfully transfers desired visual image attributes while maintaining all other attributes. t-SNE plot was used to show that encoder learns to separate different attribute values. Angle/Object: Using 3D Car and 3D Faces dataset proposed method can successfully alter the angle of input face with respect to car's angle and vice-versa.</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
	</tr>
	<tr>
		<td height="200" align="right" valign=bottom sdval="7" sdnum="1033;"><font color="#000000">7</font></td>
		<td align="center" valign=middle><font color="#000000">Deep Identity-Aware Transfer of Facial Attributes</font></td>
		<td align="center" valign=middle><u><font color="#0563C1"><a href="https://www.semanticscholar.org/paper/Deep-Identity-aware-Transfer-of-Facial-Attributes-Li-Zuo/2b507f659b341ed0f23106446de8e4322f4a3f7e">https://www.semanticscholar.org/paper/Deep-Identity-aware-Transfer-of-Facial-Attributes-Li-Zuo/2b507f659b341ed0f23106446de8e4322f4a3f7e</a></font></u></td>
		<td align="center" valign=middle><u><font color="#0563C1"><br></font></u></td>
		<td align="center" valign=middle><font color="#000000">Not specified</font></td>
		<td align="center" valign=middle sdval="2018" sdnum="1033;"><font color="#000000">2018</font></td>
		<td align="center" valign=middle><font color="#000000">Mu Li, Wangmeng Zuo, David Zhang, Jane You</font></td>
		<td align="center" valign=middle><font color="#000000">IEEE</font></td>
		<td align="center" valign=middle><font color="#000000">Transform network and Mask network. Mask network predicts to indicate the attribute relevant region. Attribute transform network is used to produce the transformed image. Transform network uses 10-layer UNet. Mask network uses 5-layer Fully Convolutional network then binarization and upsampling.</font></td>
		<td align="center" valign=middle><font color="#000000">Discriminator consist of 6 Convolutional layers and 2 Fully Connected layers at the end.</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
		<td align="center" valign=middle><font color="#000000">Adversarial attribute loss, adaptive perceptual loss, identity loss, perceptual regularization and attribute ratio regularization.</font></td>
		<td align="center" valign=middle><font color="#000000"><br></font></td>
		<td align="center" valign=middle><font color="#000000">CelebA</font></td>
		<td align="center" valign=middle><font color="#000000">Classification accuracy for attribute transfer. Identity verification accuracy. </font></td>
		<td align="center" valign=middle><font color="#000000">Attribute classification accuracy for transfer results: mouth open/0.821, mouth close/0.806, glasses removal/0.763, gender/0.684, age/0.702. Face verification accuracy ro the transfer results: mouth open/0.912, mouth close/0.903, glasses removal/0.872. </font></td>
		<td align="center" valign=middle><font color="#000000">Improve the visual quality and diversity of the transfer results, and extend model to arbitrary attribute transfer.</font></td>
	</tr>
	<tr>
		<td height="140" align="right" valign=bottom sdval="8" sdnum="1033;"><font color="#000000">8</font></td>
		<td align="center" valign=middle><font color="#000000">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</font></td>
		<td align="center" valign=middle><u><font color="#0563C1"><a href="https://www.semanticscholar.org/paper/Unpaired-Image-to-Image-Translation-Using-Networks-Zhu-Park/c43d954cf8133e6254499f3d68e45218067e4941">https://www.semanticscholar.org/paper/Unpaired-Image-to-Image-Translation-Using-Networks-Zhu-Park/c43d954cf8133e6254499f3d68e45218067e4941</a></font></u></td>
		<td align="center" valign=middle><u><font color="#0563C1"><br></font></u></td>
		<td align="center" valign=middle><u><font color="#0563C1"><a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a></font></u></td>
		<td align="center" valign=middle sdval="2017" sdnum="1033;"><font color="#000000">2017</font></td>
		<td align="center" valign=middle><font color="#000000">Jun-Yan Zhu, Taesung Park, Philip Isola, Alexei A.Efros</font></td>
		<td align="center" valign=middle><font color="#000000">Berkeley AI Research (BAIR) laboratory, UC Berkeley.</font></td>
		<td align="center" valign=middle><font color="#000000">Two generators: One with mapping function G:X-&gt;Y and F:Y-&gt;X. Residual layers used according to image size. Reflection padding used to reduce artifacts.</font></td>
		<td align="center" valign=middle><font color="#000000">Two Discriminators, Dx, Dy. 70x70 PatchGAN.</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
		<td align="center" valign=middle><font color="#000000">Adversarial loss. Cycle-consistency loss. Also added Identity loss for photo generation from paintings.</font></td>
		<td align="center" valign=middle><font color="#000000"><br></font></td>
		<td align="center" valign=middle><font color="#000000">Cityscapes training set, Images scraped from Google Maps, CMP Facade Database, UT Zappos50K, subset of ImageNET, Flickr API, art images from Wikiart.org.</font></td>
		<td align="center" valign=middle><font color="#000000">Amazon Mehcanical Turk, FCN score, Smantic segmentation metrics (per-pixel accuracy, per-class accuracy, mean class Intersection-Over-Union).</font></td>
		<td align="center" valign=middle><font color="#000000">Aerial photos-&gt;maps: AMT was fooled around quarter of time (Map-&gt;Photo/26.8%; Photo-&gt;Map/23.2%). FCN-scores labels-&gt;photo: Per-picel acc: 0.52; Per-class acc: 0.17; Class IOU: 0.11. FCN-scores photo-&gt;labels: Per-picel acc: 0.58; Per-class acc: 0.22; Class IOU: 0.16. Ablation study of loss shows that both terms are crucial for CylceGAN results. </font></td>
		<td align="center" valign=middle><font color="#000000">Handling more varied and extreme transformations, especially geometric changes. </font></td>
	</tr>
	<tr>
		<td height="220" align="right" valign=bottom sdval="9" sdnum="1033;"><font color="#000000">9</font></td>
		<td align="center" valign=middle><font color="#000000">Arbitrary style transfer in real-time with adaptive instance normalization.</font></td>
		<td align="center" valign=middle><u><font color="#0563C1"><a href="https://www.semanticscholar.org/paper/Arbitrary-Style-Transfer-in-Real-Time-with-Adaptive-Huang-Belongie/be0ef77fb0345c5851bb5d297f3ed84ae3c581ee">https://www.semanticscholar.org/paper/Arbitrary-Style-Transfer-in-Real-Time-with-Adaptive-Huang-Belongie/be0ef77fb0345c5851bb5d297f3ed84ae3c581ee</a></font></u></td>
		<td align="center" valign=middle><u><font color="#0563C1"><br></font></u></td>
		<td align="center" valign=middle><u><font color="#0563C1"><a href="https://github.com/xunhuang1995/AdaIN-style">https://github.com/xunhuang1995/AdaIN-style</a></font></u></td>
		<td align="center" valign=middle sdval="2017" sdnum="1033;"><font color="#000000">2017</font></td>
		<td align="center" valign=middle><font color="#000000">Xun Huang, Serge Belongie</font></td>
		<td align="center" valign=middle><font color="#000000">Department of Computer Science &amp; Cornell Tech, Cornell University</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
		<td align="center" valign=middle><font color="#000000">Encoder-Decoder architecture with encoder fixed to first few layers of a pre-trained VGG-19.</font></td>
		<td align="center" valign=middle><font color="#000000">Content loss and weighted style loss. </font></td>
		<td align="center" valign=middle><font color="#000000"><br></font></td>
		<td align="center" valign=middle><font color="#000000">MS-COCO as content images, dataset of paintings collected from WikiArt.</font></td>
		<td align="center" valign=middle><font color="#000000">Speed comparison. Loss values of different methods.</font></td>
		<td align="center" valign=middle><font color="#000000">Speed comparison results for 256x256 and 512x512 px images (excluding and including the style encoding procedure): 0.018 (0.027); 0.065 (0.098), number of styles - infinite. </font></td>
		<td align="center" valign=middle><font color="#000000">Explore more advanced network architectures such as the residual architecture or an architectures with additional skip connections from the encoder. Investigate more complicated training schemes like the incremental training. Replacing AdaIN with correlation alignment or histogram matching could further improve quality by transferring higher-order statistics.</font></td>
	</tr>
	<tr>
		<td height="260" align="right" valign=bottom sdval="10" sdnum="1033;"><font color="#000000">10</font></td>
		<td align="center" valign=middle><font color="#000000">Fader Networks: Manipulating Images by Sliding Attributes</font></td>
		<td align="center" valign=middle><u><font color="#0563C1"><a href="https://www.semanticscholar.org/paper/Fader-Networks%3A-Manipulating-Images-by-Sliding-Lample-Zeghidour/9d8978ee319d671283a90761aaed150c7cc9154b">https://www.semanticscholar.org/paper/Fader-Networks%3A-Manipulating-Images-by-Sliding-Lample-Zeghidour/9d8978ee319d671283a90761aaed150c7cc9154b</a></font></u></td>
		<td align="center" valign=middle><u><font color="#0563C1"><br></font></u></td>
		<td align="center" valign=middle><u><font color="#0563C1"><a href="https://github.com/facebookresearch/FaderNetworks">https://github.com/facebookresearch/FaderNetworks</a></font></u></td>
		<td align="center" valign=middle sdval="2018" sdnum="1033;"><font color="#000000">2018</font></td>
		<td align="center" valign=middle><font color="#000000">Guillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes, Ludovic Denoyer, Marc'Aurelio Ranzato.</font></td>
		<td align="center" valign=middle><font color="#000000">Facebook Research</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
		<td align="center" valign=middle><font color="#000000">Adversarial training on latent space with Discriminator which outputs probabilities of an attribute vector.</font></td>
		<td align="center" valign=middle><font color="#000000">Encoder is a convolutional neural network that maps input image to its latent representation. Encoder consists of 7 Convolution-BN-ReLU layers. Decoder is a deconvolutional network that produces version of the input image and any attribute vector. Decoder is symmetric to encoder but with transposed convolutions for upsampling.</font></td>
		<td align="center" valign=middle><font color="#000000">MSE reconstruction loss. Adversarial objective.</font></td>
		<td align="center" valign=middle><font color="#000000"><br></font></td>
		<td align="center" valign=middle><font color="#000000">CelebA</font></td>
		<td align="center" valign=middle><font color="#000000">Naturalness (measures the quality of generated images). Accuracy (measures how well swapping an attribute value is reflected in the generation). Human evaluators results are used.</font></td>
		<td align="center" valign=middle><font color="#000000">Compared to baseline IcGAN, naturalness of generated images with FadNet is more than 2 times better: for example, naturalness of generating face with glasses is FadNet/78.8 vs IcGAN/14.8.</font></td>
		<td align="center" valign=middle><font color="#000000">None</font></td>
	</tr>
	<tr>
		<td height="120" align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="center" valign=middle><font color="#000000">AttGAN: Facial Attribute Editing by<br>Only Changing What You Want</font></td>
		<td align="left" valign=bottom><u><font color="#0563C1"><a href="https://www.semanticscholar.org/paper/AttGAN%3A-Facial-Attribute-Editing-by-Only-Changing-He-Zuo/b346fa22262908c1d9dbe7ec469bfb19715e365a">https://www.semanticscholar.org/paper/AttGAN%3A-Facial-Attribute-Editing-by-Only-Changing-He-Zuo/b346fa22262908c1d9dbe7ec469bfb19715e365a</a></font></u></td>
		<td align="left" valign=bottom><u><font color="#0563C1"><br></font></u></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
	</tr>
	<tr>
		<td height="60" align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="center" valign=middle><font color="#000000">Arbitrary style transfer in real-time with adaptive instance normalization</font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
	</tr>
	<tr>
		<td height="60" align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="center" valign=middle><font color="#000000">Unsupervised Multi-Domain Image Translation with Domain-Specific Encoders/Decoders</font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
	</tr>
	<tr>
		<td height="40" align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="center" valign=middle><font color="#000000">Analyzing and Improving the Image Quality of StyleGAN</font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
		<td align="left" valign=bottom><font color="#000000"><br></font></td>
	</tr>
</table>
<!-- ************************************************************************** -->
</body>

</html>
